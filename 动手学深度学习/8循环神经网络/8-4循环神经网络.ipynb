{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回想一下，我们在 4节中 讨论过的具有隐藏单元的隐藏层。 \n",
    "# 值得注意的是，隐藏层和隐状态指的是两个截然不同的概念。 \n",
    "# 如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层\n",
    "# 而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。\n",
    "# 循环神经网络（recurrent neural networks，RNNs） 是具有隐状态的神经网络。 \n",
    "# 在介绍循环神经网络模型之前， 我们首先回顾 4.1节中介绍的多层感知机模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a657eae-d326-48e3-a449-e87611125b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前层的隐状态变量： Ht = φ(Xt * Wx + Ht-1 * Wh + bh)\n",
    "# 然后整个的 Ht作为输出：O = Ht * Whq\n",
    "# 在任意时间步t,隐状态的计算步骤：\n",
    "# 1.拼接当前时间步t的输入Xt和t-1的隐状态Ht-1\n",
    "# 2.把拼接的结果送入带有激活函数的全连接层。全连接层的输出是当前时间步t的隐状态Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c12fa4d-cdd4-4b99-b88c-f0140bcb70b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7565,  2.3581, -1.6141, -3.9876],\n",
       "        [ 2.5763,  3.3448, -2.8390,  2.0370],\n",
       "        [-3.6818, -4.9186,  1.5425, -3.7788]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58cadc9d-cb31-4074-8918-ba559826fa35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7565,  2.3581, -1.6141, -3.9876],\n",
       "        [ 2.5763,  3.3448, -2.8390,  2.0370],\n",
       "        [-3.6818, -4.9186,  1.5425, -3.7788]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在，我们沿列（轴1）拼接矩阵X和H， 沿行（轴0）拼接矩阵W_xh和W_hh。\n",
    "\n",
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\n",
    "# 也就是说，按特定方式拼接之后再dot，和先dot再相加，是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4c659-683f-44a4-9a12-6c28cfd50604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量\n",
    "# 1/n * (求和符号(t=1;n) -logP(xt|xt-1, xt-2,...x1))\n",
    "# 自然语言学家更喜欢叫做困惑度，就是在这个式子前面添加exp\n",
    "# 这样如果模型预测非常精准，困惑度为1，非常不精准，为无穷大"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (systemapp)",
   "language": "python",
   "name": "systemapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
