{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示\n",
    "# 稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。\n",
    "# 接下来，我们在corr2d函数中实现如上过程，该函数接受输入张量X和卷积核张量K，并返回输出张量Y\n",
    "\n",
    "# torch.rand(size) # 均匀分布U(0, 1)\n",
    "# torch.randn(size) # 高斯正太, random normal\n",
    "# torch.normal(mean, std, size)\n",
    "# torch.arange(start, end, step)\n",
    "# torch.FloatTensor([]) 创建一个float32的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10f5938-1504-4455-baed-68436c4b8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# correlation 2d 互相关运算-2d\n",
    "def corr2d(X, K): #@save\n",
    "    # 计算二维互相关运算\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1 )) # 输出矩阵大小\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i+h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2319a1a-d91f-4435-8a5b-ea96d8122b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], \n",
    "                  [3.0, 4.0, 5.0], \n",
    "                  [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], \n",
    "                  [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a95e58-3ae0-433c-b4d2-a428c3e984b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 \n",
    "# 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d0fb5a-9457-4008-91f0-ef70841ba0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Paramater(torch.rand(kernel_size)) # 卷积核权重\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a4bd97-d5b0-4359-aee8-ba2e09388cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44700f20-51d2-4e79-9c40-aac23b2de6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edfbdf2-6855-41f5-8a4e-dc84d9834a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7dd763b-36c0-4f6d-91b7-b9f8aa0614d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将输入的二维图像转置，再进行如上的互相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6de8f89-b83e-4db5-9861-ff6b81ba57fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K) # 垂直边缘消失了，不出所料，只能检测垂直边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ea0074-ebac-476d-bcb4-63a423954df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在让我们看看是否可以通过仅查看“输入-输出”对来学习由X生成Y的卷积核。 \n",
    "# 我们先构造一个卷积层，并将其卷积核初始化为随机张量。\n",
    "# 接下来，在每次迭代中，我们比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。\n",
    "# 为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ca028a-095c-4534-9df1-b0f6bc8e0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 6.183\n",
      "epoch 4, loss 1.969\n",
      "epoch 6, loss 0.712\n",
      "epoch 8, loss 0.276\n",
      "epoch 10, loss 0.110\n",
      "epoch 12, loss 0.045\n",
      "epoch 14, loss 0.018\n",
      "epoch 16, loss 0.007\n",
      "epoch 18, loss 0.003\n",
      "epoch 20, loss 0.001\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias = False) # in_channels, out_channels, size\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # 学习率\n",
    "\n",
    "for i in range(20):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d14c45ea-1a26-479f-8e46-52f0e9337424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们这里是在让这个conv2D 学习我们先前算出来的Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42cf772f-2a04-40af-8a40-f47ca0e6f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9963, -1.0036]], grad_fn=<ViewBackward0>),\n",
       " tensor([[ 0.9963, -1.0036]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.view(1, -1), conv2d.weight.data.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cf887-036b-4d49-91e5-f27945e5fe7e",
   "metadata": {},
   "source": [
    "##### 接下来介绍填充和步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2793e9f-9564-4d1c-a66a-7f6fa52cb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我们图像为 nh,nw ; 卷积核 kh,kw ; 我们填充 ph,pw(指一半在上面一半下面，一共ph)\n",
    "# 那一般来说我们的输出形状就会变成 (nh + ph - kh + 1) * (nw + pw - kw + 1)\n",
    "# 一般来说 我们设置 ph = kh - 1 ; pw = kw - 1 \n",
    "# k选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876e021d-647d-4632-9895-73103daee35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了方便起见，我们定义了一个计算卷积层的函数。\n",
    "# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # 这里的（1，1）表示批量大小和通道数都是1\n",
    "    X = X.reshape((1, 1) + X.shape) # 元组的链接操作\n",
    "    Y = conv2d(X)\n",
    "    # 省略前两个维度：批量大小和通道s\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size = 3, padding = 1)\n",
    "X = torch.rand(size = (8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d762d7d-5ff7-4b08-8453-62967972a154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 而当有步幅 sh,sw时，输出形状就变成 [(nh + ph - kh + sh) / sh ] * [(nw + pw - kw + sw) / sw]\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size = 3, padding = 1, stride = 2)\n",
    "comp_conv2d(conv2d, X).shape\n",
    "# 长: 8 + 2 - 3 + 2 / 2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba9ddb-f550-4c79-9217-3a98ed56eb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (systemapp)",
   "language": "python",
   "name": "systemapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
