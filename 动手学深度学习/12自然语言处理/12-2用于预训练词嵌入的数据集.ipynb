{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566deb7c-41ff-479f-9964-9fb4e340ad6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences数: 42069'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
    "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "\n",
    "#@save\n",
    "def read_ptb():\n",
    "    \"\"\"将PTB数据集加载到文本行的列表中\"\"\"\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Readthetrainingset.\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()\n",
    "f'# sentences数: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446713da-8fdc-484a-8181-9ef9a1990fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 6719'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "f'vocab size: {len(vocab)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c42c66-9fea-45b3-b37c-864e3f108031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们用下采样，让频率越高的词，在skip-gram之前，被丢掉的概率越大\n",
    "# p = max(0, math.sqrt(1/f(counter))) f算的是这个词在所有词中出现的比例\n",
    "\n",
    "#@save\n",
    "def subsample(sentences, vocab):\n",
    "    \"\"\"下采样高频词\"\"\"\n",
    "    # 排除未知词元'<unk>'\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                 for line in sentences]\n",
    "    counter = d2l.count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # 如果在下采样期间保留词元，则返回True\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) <\n",
    "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences],\n",
    "            counter)\n",
    "\n",
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58d8642-59b6-4b4d-b394-183afd827e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"the\"的数量：之前=50770, 之后=2025'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'\"{token}\"的数量：'\n",
    "            f'之前={sum([l.count(token) for l in sentences])}, '\n",
    "            f'之后={sum([l.count(token) for l in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a464e5aa-f625-40e1-ba8d-3d4ea5f79a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"join\"的数量：之前=45, 之后=45'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778bcad8-4f54-4c29-8a58-0c1f530ac281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到低频词都被保留了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9ec7d6-18e0-429c-8bea-bf6af9a7885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [392, 2115, 145, 274, 406], [5277, 3054, 1580]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5d2a5e-50e4-4e15-8a61-fb0d6b0fa32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的get_centers_and_contexts函数从corpus中提取所有中心词及其上下文词。\n",
    "# 它随机采样1到max_window_size之间的整数作为上下文窗口。\n",
    "\n",
    "#@save\n",
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"返回跳元模型中的中心词和上下文词\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # 要形成“中心词-上下文词”对，每个句子至少需要有2个词\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line # 一句话中的每个词都有机会成为中心词\n",
    "        # print(\"center:\", centers)\n",
    "        for i in range(len(line)):  # 上下文窗口中间i\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size))) # 存位置\n",
    "            # 从上下文词中排除中心词\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "            # print('context:',contexts)\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6663b17f-1141-4b90-90d3-95e5a01ed736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，我们创建一个人工数据集，分别包含7个和3个单词的两个句子。\n",
    "# 设置最大上下文窗口大小为2，并打印所有中心词及其上下文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c8e11d-8450-409b-9a21-34c5d69b22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集 [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "中心词 0 的上下文词是 [1]\n",
      "中心词 1 的上下文词是 [0, 2, 3]\n",
      "中心词 2 的上下文词是 [0, 1, 3, 4]\n",
      "中心词 3 的上下文词是 [1, 2, 4, 5]\n",
      "中心词 4 的上下文词是 [3, 5]\n",
      "中心词 5 的上下文词是 [3, 4, 6]\n",
      "中心词 6 的上下文词是 [4, 5]\n",
      "中心词 7 的上下文词是 [8, 9]\n",
      "中心词 8 的上下文词是 [7, 9]\n",
      "中心词 9 的上下文词是 [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('数据集', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('中心词', center, '的上下文词是', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5674df5e-bc58-4898-a83a-8411f8f5df36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# “中心词-上下文词对”的数量: 1502704'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# “中心词-上下文词对”的数量: {sum([len(contexts) for contexts in all_contexts])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864fcdba-2c20-4bb8-b25d-1da458f107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用负采样进行近似训练。为了根据预定义的分布对噪声词进行采样，我们定义以下RandomGenerator类\n",
    "# 其中（可能未规范化的）采样分布通过变量sampling_weights传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf29c9f-7956-4c5b-a602-cdf28fda3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class RandomGenerator:\n",
    "    # population 所有词的id\n",
    "    # weights 负采样的权重\n",
    "    # candidates 缓存池\n",
    "    \"\"\"根据n个采样权重在{1,...,n}中随机抽取\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        # Exclude\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # 缓存k个随机采样结果\n",
    "            self.candidates = random.choices(\n",
    "                self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1] # 一次缓存10000个，但是一次只返回一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f876351d-8b27-436a-afd8-32c94f51ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 2, 2, 3, 1, 3, 2, 3, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "generator = RandomGenerator([2, 3, 4])\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdca6083-7c6e-4ea1-8c3f-4dbe799dd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于一对中心词和上下文词，我们随机抽取了K个（实验中为5个）噪声词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c500a79-9b67-44d3-888a-99db9990ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_negatives(all_contexts, vocab, counter, K): # k 就是取多少个噪声词\n",
    "    # all_context 指的是所有的上下文列表\n",
    "    # counter 算的是出现的频率，负采样的频率和词出现的频率的3/4次方正相关\n",
    "    \"\"\"返回负采样中的噪声词\"\"\"\n",
    "    # 索引为1、2、...（索引0是词表中排除的未知标记）\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
    "                        for i in range(1, len(vocab))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # 噪声词不能是上下文词\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57db89a5-03a1-4562-a6eb-b35e2a12a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def batchify(data): # data (center, context, negative)\n",
    "    \"\"\"返回带有负采样的跳元模型的小批量样本\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += \\\n",
    "            [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
    "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e76752a-19da-4ded-af7f-266ac7da9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 3, 0]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd76afc1-29c7-404e-937d-bee52cbc2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2948655a-e2d8-442f-9bbb-b225b5799bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    \"\"\"下载PTB数据集，然后将其加载到内存中\"\"\"\n",
    "    num_workers = get_dataloader_workers()\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(\n",
    "        corpus, max_window_size)\n",
    "    all_negatives = get_negatives(\n",
    "        all_contexts, vocab, counter, num_noise_words)\n",
    "\n",
    "    class PTBDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives)\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index],\n",
    "                    self.negatives[index])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "\n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, shuffle=True,\n",
    "        collate_fn=batchify, num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830d085b-1a78-4568-a05c-94ee065a4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "data_iter, vocab = load_data_ptb(512, 5, 5)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b33f5-6848-4284-adb9-e2c593d0460b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (systemapp)",
   "language": "python",
   "name": "systemapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
