{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前的介绍中，我们只依靠深度学习框架来完成训练的工作， 而忽略了操作参数的具体细节。 本节，我们将介绍以下内容：\n",
    "# 1.访问参数，调试，诊断，优化\n",
    "# 2.参数初始化\n",
    "# 3.在不同组件中共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe711a76-e690-4c5b-8985-a4180d08121c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1776],\n",
       "        [-0.1717]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 隐藏层的定义可以是：神经网络中不直接对应输入和输出的中间层\n",
    "# 也就是说，这里的8的这一层就是单个的隐藏层\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), # layer 0\n",
    "    nn.ReLU(), # layer 1\n",
    "    nn.Linear(8, 1)) # layer 2\n",
    "X = torch.rand(2, 4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cf1b66-fedf-44a6-a82e-569b768d6b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1909, -0.3056, -0.1852, -0.2586,  0.2490, -0.2299,  0.2318,  0.1708]])), ('bias', tensor([0.0422]))])\n",
      "Linear(in_features=4, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 我们从已有模型中访问参数。 当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 \n",
    "# 这就像模型是一个列表一样，每层的参数都在其属性中。 如下所示，我们可以检查第二个全连接层的参数\n",
    "# state_dict 是 PyTorch 中所有 nn.Module 的内置方法, 用来查看参数， 保存模型， 加载模型\n",
    "# 访问_modules, 这是一个Ordered_Dict\n",
    "\n",
    "print(net[2].state_dict())\n",
    "# 中括号是索引, 小括号是函数调用\n",
    "print(net._modules[\"0\"]) # 不建议用_modules查看参数，这是内部 API，未来 PyTorch 版本可能改变，不应该依赖pytorch的内部api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d8795e-63bf-492e-b27f-db463ee14d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0422], requires_grad=True)\n",
      "tensor([0.0422])\n"
     ]
    }
   ],
   "source": [
    "# 注意，每个参数都表示为参数类的一个实例。 \n",
    "# 要对参数执行任何操作，首先我们需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "# 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。\n",
    "\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb1431c-81e9-4c88-9570-e67992d6420d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 \n",
    "# 除了值之外，我们还可以访问每个参数的梯度。 \n",
    "# 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n",
    "\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd896af8-36e1-4f7d-860a-1408ecc73a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。 \n",
    "# 当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数。 \n",
    "# 下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n",
    "\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "# named_parameters返回这个层的参数和名字\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d905341-c09b-4d23-a111-e73018c119ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0422]),\n",
       " OrderedDict([('0.weight',\n",
       "               tensor([[-0.3647, -0.2277, -0.1160,  0.0188],\n",
       "                       [-0.2251, -0.1569, -0.0604, -0.3514],\n",
       "                       [ 0.0318,  0.4238, -0.1661,  0.0714],\n",
       "                       [-0.4852,  0.4074,  0.3562,  0.3262],\n",
       "                       [ 0.2718,  0.3372, -0.0144,  0.2312],\n",
       "                       [ 0.4786,  0.1589,  0.1778,  0.0387],\n",
       "                       [ 0.1857, -0.4370, -0.4006,  0.1721],\n",
       "                       [ 0.1909, -0.0787, -0.1145,  0.1738]])),\n",
       "              ('0.bias',\n",
       "               tensor([-0.4424,  0.0194,  0.1450, -0.2114, -0.4539,  0.0959, -0.4844,  0.1643])),\n",
       "              ('2.weight',\n",
       "               tensor([[-0.1909, -0.3056, -0.1852, -0.2586,  0.2490, -0.2299,  0.2318,  0.1708]])),\n",
       "              ('2.bias', tensor([0.0422]))]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'], net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18abb657-a898-43c4-b9b9-58bf56f69778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2628],\n",
       "        [-0.2628]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。 我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n",
    "# X = rand(2, 4)\n",
    "\n",
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 4),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1)) # 很明显，第一层net[0] = block2 , 第二层net[1] = Linear(4, 1)\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17da719-0931-4bd4-adc9-63e34c7cc65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "分割线\n",
      "Parameter containing:\n",
      "tensor([[ 0.0424,  0.3936, -0.3690, -0.4581],\n",
      "        [ 0.1781,  0.0720,  0.3786, -0.2982],\n",
      "        [ 0.0484,  0.2678,  0.2782,  0.2176],\n",
      "        [ 0.1811, -0.1162,  0.2662,  0.2276],\n",
      "        [ 0.3050, -0.1601,  0.2828,  0.1299],\n",
      "        [-0.3542,  0.0042, -0.2979,  0.1460],\n",
      "        [ 0.3720,  0.1161,  0.1288,  0.3819],\n",
      "        [ 0.4354,  0.3987,  0.2078,  0.1961]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2930, -0.2657, -0.3516, -0.2095], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)\n",
    "print(\"分割线\")\n",
    "# 如果是regnet[0][0].bias会报错，因为这样找的是一个Sequential，而Sequential没有bias属性，会报错\n",
    "print(rgnet[0][0][0].weight)\n",
    "print(rgnet[0][0][2].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f768890-a4b3-4923-a62a-57bd533df7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知道了如何访问参数后，现在我们看看如何正确地初始化参数\n",
    "# 深度学习框架提供默认随机初始化， 也允许我们创建自定义初始化方法， 满足我们通过其他规则实现初始化权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d5766a-7f8f-46fd-b0b2-dda8c6aa7627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0101, -0.0028,  0.0061,  0.0039],\n",
       "         [ 0.0190,  0.0159, -0.0042, -0.0120],\n",
       "         [-0.0034, -0.0102, -0.0216, -0.0031],\n",
       "         [-0.0079, -0.0060, -0.0137, -0.0081],\n",
       "         [-0.0091,  0.0074,  0.0100,  0.0185],\n",
       "         [ 0.0076, -0.0064, -0.0034, -0.0096],\n",
       "         [ 0.0130,  0.0137,  0.0157, -0.0043],\n",
       "         [-0.0036,  0.0048,  0.0093, -0.0092]], requires_grad=True),\n",
       " tensor(0., grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 内置初始化\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean = 0, std = 0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight, net[0].bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87823d97-da29-4539-820b-2f513dd4243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n",
    "\n",
    "def init_constant1(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant1)\n",
    "net[0].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d506d1e3-04b0-4622-a773-d073cc19fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4505,  0.6658,  0.6187, -0.3078])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以对某些块应用不同的初始化方法。 \n",
    "# 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。\n",
    "\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8858ba3-e42e-4752-8eaf-53de0d54004b",
   "metadata": {},
   "source": [
    "##### 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23602873-b27b-4666-99da-daaa69f5309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "INIT ('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000, -9.7328],\n",
       "        [-6.7907, -7.9620, -0.0000, -8.7190]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # * 在函数定义中，为打包成元组，比如def a(*args) ; 在函数调用中，表示解包成多个参数\n",
    "        print(\"INIT\", *[(name, param.shape) for name, param in m.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        with torch.no_grad():\n",
    "            m.weight *= (m.weight.abs() >= 5)\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27aae807-936e-408f-9716-6334de6ccd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1741, -0.1932,  0.0307, -0.1423,  0.0371, -0.3061, -0.0268, -0.0719],\n",
      "        [ 0.0393,  0.0006,  0.2796, -0.2513, -0.3124,  0.2374, -0.0149,  0.0736],\n",
      "        [ 0.1997, -0.1282, -0.1059,  0.2225,  0.2771,  0.3130,  0.1038, -0.2906],\n",
      "        [-0.2793,  0.2147,  0.3526,  0.2102, -0.2894,  0.1419, -0.2685,  0.1945],\n",
      "        [ 0.0266, -0.2051,  0.1307, -0.2139,  0.0129, -0.3394,  0.1733,  0.1424],\n",
      "        [-0.1078,  0.0036, -0.0895, -0.3350,  0.3499,  0.1938, -0.2457, -0.2178],\n",
      "        [-0.0190,  0.0541, -0.2866, -0.1933,  0.1341, -0.0320,  0.1166, -0.0659],\n",
      "        [ 0.0364,  0.2059,  0.1534, -0.0258,  0.3113,  0.1280,  0.2600,  0.0470]],\n",
      "       requires_grad=True)\n",
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n",
    "\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight[0] == net[4].weight[0])\n",
    "net[2].weight.data[0, 0] = 100 # [0, 0]是对一个矩阵的直接索引；[0][0]是链式索引；两者结果一样的\n",
    "# # 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight[0] == net[4].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a6df1-3299-45ff-bdbe-4eecbc75a228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (systemapp)",
   "language": "python",
   "name": "systemapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
